{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(1)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(1, torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# Create the transformation pipeline\n",
    "transform= T.Compose([\n",
    "    T.Lambda(lambda img: img.convert(\"RGB\")),  # Ensure image is in RGB.\n",
    "    T.Resize(336, interpolation=T.InterpolationMode.BICUBIC),  # Resize: shortest edge = 336.\n",
    "    T.CenterCrop((336, 336)),  # Center crop to 336x336.\n",
    "    T.ToTensor(),  # Convert to tensor and scale pixels to [0, 1] (i.e. multiply by rescale_factor 0.00392).\n",
    "])\n",
    "normalize = T.Normalize(\n",
    "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "        std=[0.26862954, 0.26130258, 0.27577711]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MinBackward1>) tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor([[[[ 0.5435,  0.6455,  0.5581,  ...,  0.0909,  0.0033, -0.0696],\n",
      "          [ 0.5435,  0.6167,  0.5435,  ...,  0.1201,  0.0179,  0.0617],\n",
      "          [ 0.5581,  0.5581,  0.6602,  ...,  0.0909,  0.0764,  0.0617],\n",
      "          ...,\n",
      "          [ 1.8281,  1.8867,  1.8281,  ...,  1.4053,  1.4482,  1.5654],\n",
      "          [ 1.8574,  1.9014,  1.8721,  ...,  1.4775,  1.4053,  1.4922],\n",
      "          [ 1.8721,  1.9014,  1.9014,  ...,  1.4053,  1.2148,  1.4775]],\n",
      "\n",
      "         [[-1.3623, -1.2715, -1.3770,  ..., -1.4219, -1.4824, -1.5117],\n",
      "          [-1.3320, -1.2422, -1.3467,  ..., -1.4219, -1.4824, -1.4219],\n",
      "          [-1.2422, -1.2871, -1.1973,  ..., -1.4668, -1.4668, -1.4824],\n",
      "          ...,\n",
      "          [ 0.0789,  0.1239,  0.0338,  ..., -0.7168, -0.6567, -0.5664],\n",
      "          [ 0.1089,  0.1089,  0.0789,  ..., -0.6265, -0.7168, -0.6265],\n",
      "          [ 0.1239,  0.1089,  0.0789,  ..., -0.6416, -0.8818, -0.5513]],\n",
      "\n",
      "         [[-0.5562, -0.3853, -0.4138,  ..., -0.8687, -0.8545, -0.8687],\n",
      "          [-0.4563, -0.4421, -0.4849,  ..., -0.8120, -0.8828, -0.7832],\n",
      "          [-0.5273, -0.4421, -0.3994,  ..., -0.8687, -0.8262, -0.8403],\n",
      "          ...,\n",
      "          [ 1.6055,  1.5771,  1.5625,  ...,  0.8521,  0.7666,  0.8091],\n",
      "          [ 1.6055,  1.6621,  1.6621,  ...,  0.7808,  0.8662,  0.6670],\n",
      "          [ 1.6484,  1.6484,  1.6621,  ...,  0.8379,  0.8945,  0.8232]]]],\n",
      "       device='cuda:1', dtype=torch.float16)\n",
      "tensor(-1.7920, device='cuda:1', dtype=torch.float16) tensor(2.1465, device='cuda:1', dtype=torch.float16)\n",
      "torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "image = T.ToTensor()(raw_image)\n",
    "image.requires_grad = True\n",
    "print(image.min(), image.max())\n",
    "\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(1, torch.float16)[\"pixel_values\"]\n",
    "\n",
    "# print(processor.__dict__)\n",
    "# print(dir(processor))\n",
    "print(inputs)\n",
    "print(inputs.min(), inputs.max())\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = transform(raw_image)\n",
    "image.requires_grad = True\n",
    "processed_image = normalize(image).to(1, torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5435,  0.6455,  0.5581,  ...,  0.0909,  0.0033, -0.0696],\n",
      "         [ 0.5435,  0.6167,  0.5435,  ...,  0.1201,  0.0179,  0.0617],\n",
      "         [ 0.5581,  0.5581,  0.6602,  ...,  0.0909,  0.0764,  0.0617],\n",
      "         ...,\n",
      "         [ 1.8281,  1.8867,  1.8281,  ...,  1.4053,  1.4482,  1.5654],\n",
      "         [ 1.8574,  1.9014,  1.8721,  ...,  1.4775,  1.4053,  1.4922],\n",
      "         [ 1.8721,  1.9014,  1.9014,  ...,  1.4053,  1.2148,  1.4775]],\n",
      "\n",
      "        [[-1.3623, -1.2715, -1.3770,  ..., -1.4219, -1.4824, -1.5117],\n",
      "         [-1.3320, -1.2422, -1.3467,  ..., -1.4219, -1.4824, -1.4219],\n",
      "         [-1.2422, -1.2871, -1.1973,  ..., -1.4668, -1.4668, -1.4824],\n",
      "         ...,\n",
      "         [ 0.0789,  0.1239,  0.0338,  ..., -0.7168, -0.6567, -0.5664],\n",
      "         [ 0.1089,  0.1089,  0.0789,  ..., -0.6265, -0.7168, -0.6265],\n",
      "         [ 0.1239,  0.1089,  0.0789,  ..., -0.6416, -0.8818, -0.5513]],\n",
      "\n",
      "        [[-0.5562, -0.3853, -0.4138,  ..., -0.8687, -0.8545, -0.8687],\n",
      "         [-0.4563, -0.4421, -0.4849,  ..., -0.8120, -0.8828, -0.7832],\n",
      "         [-0.5273, -0.4421, -0.3994,  ..., -0.8687, -0.8262, -0.8403],\n",
      "         ...,\n",
      "         [ 1.6055,  1.5771,  1.5625,  ...,  0.8521,  0.7666,  0.8091],\n",
      "         [ 1.6055,  1.6621,  1.6621,  ...,  0.7808,  0.8662,  0.6670],\n",
      "         [ 1.6484,  1.6484,  1.6621,  ...,  0.8379,  0.8945,  0.8232]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(-1.7920, device='cuda:1', dtype=torch.float16, grad_fn=<MinBackward1>) tensor(2.1465, device='cuda:1', dtype=torch.float16, grad_fn=<MaxBackward1>)\n",
      "torch.Size([3, 336, 336])\n",
      "tensor(0., device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(processed_image)\n",
    "print(processed_image.min(), processed_image.max())\n",
    "print(processed_image.shape)\n",
    "\n",
    "print(torch.norm(processed_image - inputs))\n",
    "\n",
    "print(processed_image.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n",
      "torch.Size([1, 3, 336, 336])\n",
      "tensor(-1.7920, device='cuda:1', dtype=torch.float16) tensor(2.1465, device='cuda:1', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "What are these? ASSISTANT: These are two cats lying on a pink couch.\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs['pixel_values'].shape)\n",
    "print(inputs[\"pixel_values\"].min(), inputs[\"pixel_values\"].max())\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
