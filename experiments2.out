Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  5.95it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  6.89it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.31it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.07it/s]
2025-03-03 17:56:40,312 - INFO - --- Starting Experiment: New minimum search width 1 ---
2025-03-03 17:56:40 [gcg.py:222] Tokenizer does not have a chat template. Assuming base model and setting chat template to empty.
2025-03-03 17:56:40,826 - WARNING - Tokenizer does not have a chat template. Assuming base model and setting chat template to empty.
/raid/users/dlopez/miniconda3/envs/myenv39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:159: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
/raid/users/dlopez/miniconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)
  return F.linear(input, self.weight, self.bias)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  0%|          | 0/500 [00:00<?, ?it/s]/raid/users/dlopez/DS-GCG/nanoGCG/nanogcg/gcg.py:595: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)
  optim_embeds = optim_ids_onehot @ embedding_layer.weight
/raid/users/dlopez/miniconda3/envs/myenv39/lib/python3.9/site-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/raid/users/dlopez/miniconda3/envs/myenv39/lib/python3.9/site-packages/torch/autograd/graph.py:823: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:102.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-03-03 17:56:43 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:43,142 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  0%|          | 1/500 [00:04<37:55,  4.56s/it]2025-03-03 17:56:47 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:47,502 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  0%|          | 2/500 [00:06<24:20,  2.93s/it]2025-03-03 17:56:49 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:49,223 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  1%|          | 3/500 [00:08<20:23,  2.46s/it]2025-03-03 17:56:51 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:51,123 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  1%|          | 4/500 [00:10<18:34,  2.25s/it]2025-03-03 17:56:53 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:53,056 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  1%|          | 5/500 [00:12<17:47,  2.16s/it]2025-03-03 17:56:55 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:55,050 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  1%|          | 6/500 [00:14<17:37,  2.14s/it]2025-03-03 17:56:57 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:57,135 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  1%|▏         | 7/500 [00:16<17:05,  2.08s/it]2025-03-03 17:56:59 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:56:59,093 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  2%|▏         | 8/500 [00:18<17:44,  2.16s/it]2025-03-03 17:57:01 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:01,521 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  2%|▏         | 9/500 [00:20<18:10,  2.22s/it]2025-03-03 17:57:03 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:03,775 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  2%|▏         | 10/500 [00:25<22:50,  2.80s/it]2025-03-03 17:57:07 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:07,900 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  2%|▏         | 11/500 [00:27<21:42,  2.66s/it]2025-03-03 17:57:10 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:10,271 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  2%|▏         | 12/500 [00:29<20:17,  2.50s/it]2025-03-03 17:57:12 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:12,458 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  3%|▎         | 13/500 [00:31<19:10,  2.36s/it]2025-03-03 17:57:14 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:14,452 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  3%|▎         | 14/500 [00:33<18:20,  2.26s/it]2025-03-03 17:57:16 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:16,488 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  3%|▎         | 15/500 [00:35<17:49,  2.20s/it]2025-03-03 17:57:18 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:18,556 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  3%|▎         | 16/500 [00:37<17:17,  2.14s/it]2025-03-03 17:57:20 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:20,758 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  3%|▎         | 17/500 [00:39<17:33,  2.18s/it]2025-03-03 17:57:22 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:22,927 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  4%|▎         | 18/500 [00:42<18:37,  2.32s/it]2025-03-03 17:57:25 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:25,511 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  4%|▍         | 19/500 [00:44<17:37,  2.20s/it]2025-03-03 17:57:27 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:27,320 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  4%|▍         | 20/500 [00:46<16:33,  2.07s/it]2025-03-03 17:57:29 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:29,101 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  4%|▍         | 21/500 [00:48<16:22,  2.05s/it]2025-03-03 17:57:31 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:31,241 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  4%|▍         | 22/500 [00:50<16:08,  2.03s/it]2025-03-03 17:57:33 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:33,094 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  5%|▍         | 23/500 [00:52<15:41,  1.97s/it]2025-03-03 17:57:34 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:34,993 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  5%|▍         | 24/500 [00:53<15:18,  1.93s/it]2025-03-03 17:57:36 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:36,832 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  5%|▌         | 25/500 [00:55<15:08,  1.91s/it]2025-03-03 17:57:38 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:38,685 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  5%|▌         | 26/500 [00:57<14:48,  1.87s/it]2025-03-03 17:57:40 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:40,464 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  5%|▌         | 27/500 [00:59<15:13,  1.93s/it]2025-03-03 17:57:42 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:42,578 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  6%|▌         | 28/500 [01:01<15:09,  1.93s/it]2025-03-03 17:57:44 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:44,447 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  6%|▌         | 29/500 [01:03<14:58,  1.91s/it]2025-03-03 17:57:46 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:46,327 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  6%|▌         | 30/500 [01:05<14:53,  1.90s/it]2025-03-03 17:57:48 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:48,177 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  6%|▌         | 31/500 [01:07<14:42,  1.88s/it]2025-03-03 17:57:50 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:50,145 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  6%|▋         | 32/500 [01:09<15:40,  2.01s/it]2025-03-03 17:57:52 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:52,410 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  7%|▋         | 33/500 [01:11<15:37,  2.01s/it]2025-03-03 17:57:54 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:54,362 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  7%|▋         | 34/500 [01:13<15:12,  1.96s/it]2025-03-03 17:57:56 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:56,213 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  7%|▋         | 35/500 [01:15<14:52,  1.92s/it]2025-03-03 17:57:58 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:58,025 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  7%|▋         | 36/500 [01:16<14:28,  1.87s/it]2025-03-03 17:57:59 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:57:59,740 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  7%|▋         | 37/500 [01:18<14:18,  1.85s/it]2025-03-03 17:58:01 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:58:01,596 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  8%|▊         | 38/500 [01:20<14:07,  1.83s/it]2025-03-03 17:58:03 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:58:03,354 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  8%|▊         | 39/500 [01:22<14:04,  1.83s/it]2025-03-03 17:58:05 [gcg.py:313] torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
2025-03-03 17:58:05,181 - WARNING - torch.Size([1, 20, 32064]) torch.Size([1, 3, 336, 336])
  8%|▊         | 39/500 [01:23<16:26,  2.14s/it]
Decreasing batch size to: 247
Decreasing batch size to: 236
Traceback (most recent call last):
  File "/raid/users/dlopez/DS-GCG/experiments.py", line 123, in <module>
    run_experiment(
  File "/raid/users/dlopez/DS-GCG/experiments.py", line 77, in run_experiment
    result = nanogcg.run(model, tokenizer, message, target, config, transform = transform, normalize = normalize)
  File "/raid/users/dlopez/DS-GCG/nanoGCG/nanogcg/gcg.py", line 721, in run
    return result
  File "/raid/users/dlopez/DS-GCG/nanoGCG/nanogcg/gcg.py", line 394, in run
    self._compute_candidates_loss_original, batch_size
  File "/raid/users/dlopez/DS-GCG/nanoGCG/nanogcg/utils.py", line 105, in decorator
    return function(batch_size, *args, **kwargs)
  File "/raid/users/dlopez/DS-GCG/nanoGCG/nanogcg/gcg.py", line 700, in _compute_candidates_loss_original
    torch.cuda.empty_cache()
KeyboardInterrupt
